{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gan-getting-started.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c gan-getting-started\n",
    "\n",
    "ROOT = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"data/gan-getting-started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_path = 'gan-getting-started.zip'\n",
    "extract_path = 'data/gan-getting-started'  # Optional: Specify a target directory\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)  # Extracts to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_RATIO = 0.8\n",
    "\n",
    "# data_dir = os.path.join(ROOT, 'CUB_200_2011')\n",
    "# images_dir = os.path.join(data_dir, 'images')\n",
    "# train_dir = os.path.join(data_dir, 'train')\n",
    "# test_dir = os.path.join(data_dir, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Config:\n",
    "    content_dir = \"data/gan-getting-started/photo_jpg\"\n",
    "    style_dir = \"data/gan-getting-started/monet_jpg\"\n",
    "    image_size = 256\n",
    "    batch_size = 4\n",
    "    epochs = 1000\n",
    "    save_every = 100\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                         \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, content_dir, style_dir, transform=None, image_size=256):\n",
    "        self.content_dir = content_dir\n",
    "        self.style_dir = style_dir\n",
    "        self.transform = transform or self.default_transform(image_size)\n",
    "        self.content_images = [f for f in os.listdir(content_dir) if f.endswith(('.jpg', '.png'))]\n",
    "        self.style_images = [f for f in os.listdir(style_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    @staticmethod\n",
    "    def default_transform(image_size):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.content_images), len(self.style_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content_idx = idx % len(self.content_images)\n",
    "        style_idx = idx % len(self.style_images)\n",
    "        \n",
    "        content_img = Image.open(os.path.join(self.content_dir, self.content_images[content_idx])).convert('RGB')\n",
    "        style_img = Image.open(os.path.join(self.style_dir, self.style_images[style_idx])).convert('RGB')\n",
    "        \n",
    "        return self.transform(content_img), self.transform(style_img)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ReflectionPad2d(padding),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, stride=1),\n",
    "            ConvBlock(channels, channels, kernel_size=3, stride=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initial layers\n",
    "        self.conv1 = ConvBlock(3, 32, kernel_size=9, stride=1)\n",
    "        self.conv2 = ConvBlock(32, 64, kernel_size=3, stride=2)\n",
    "        self.conv3 = ConvBlock(64, 128, kernel_size=3, stride=2)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(128) for _ in range(5)])\n",
    "        \n",
    "        # Upsampling\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ConvBlock(128, 64, kernel_size=3, stride=1)\n",
    "        )\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            ConvBlock(64, 32, kernel_size=3, stride=1)\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Sequential(\n",
    "            nn.ReflectionPad2d(4),\n",
    "            nn.Conv2d(32, 3, kernel_size=9),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:7]).eval()\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class StyleTransferLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    @staticmethod\n",
    "    def gram_matrix(x):\n",
    "        b, c, h, w = x.size()\n",
    "        features = x.view(b * c, h * w)\n",
    "        gram = torch.mm(features, features.t())\n",
    "        return gram.div(b * c * h * w)\n",
    "    \n",
    "    def forward(self, generated, content, style):\n",
    "        # Ensure all inputs are 4D\n",
    "        if generated.dim() == 3:\n",
    "            generated = generated.unsqueeze(0)\n",
    "        if content.dim() == 3:\n",
    "            content = content.unsqueeze(0)\n",
    "        if style.dim() == 3:\n",
    "            style = style.unsqueeze(0)\n",
    "        \n",
    "        # Extract features\n",
    "        gen_features = self.feature_extractor(generated)\n",
    "        content_features = self.feature_extractor(content)\n",
    "        style_features = self.feature_extractor(style)\n",
    "        \n",
    "        # Content loss\n",
    "        content_loss = self.mse_loss(gen_features, content_features)\n",
    "        \n",
    "        # Style loss\n",
    "        style_loss = 0\n",
    "        for gen_f, style_f in zip([gen_features], [style_features]):  # Using single layer for simplicity\n",
    "            style_loss += self.mse_loss(self.gram_matrix(gen_f), self.gram_matrix(style_f))\n",
    "        \n",
    "        return content_loss + 1e5 * style_loss\n",
    "\n",
    "def train():\n",
    "    config = Config()\n",
    "    print(f\"Using device: {config.device}\")\n",
    "    \n",
    "    # Dataset and DataLoader\n",
    "    dataset = StyleTransferDataset(config.content_dir, config.style_dir, image_size=config.image_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    \n",
    "    # Model and optimizer\n",
    "    transformer = TransformerNet().to(config.device)\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Loss function\n",
    "    feature_extractor = FeatureExtractor().to(config.device)\n",
    "    criterion = StyleTransferLoss(feature_extractor)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{config.epochs}\")\n",
    "        for content_imgs, style_imgs in pbar:\n",
    "            # Move data to device\n",
    "            content_imgs = content_imgs.to(config.device)\n",
    "            style_imgs = style_imgs.to(config.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            generated = transformer(content_imgs)\n",
    "            loss = criterion(generated, content_imgs, style_imgs)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pbar.set_postfix(loss=f\"{loss.item():.2f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if epoch % config.save_every == 0:\n",
    "            torch.save(transformer.state_dict(), f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    \n",
    "    torch.save(transformer.state_dict(), \"final_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/1000:   0%|          | 0/1760 [00:00<?, ?it/s]0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Epoch 1/1000:  29%|██▉       | 509/1760 [00:35<01:23, 15.07it/s, loss=0.01]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m torch.save(\u001b[43mmodel\u001b[49m.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mfinal_style_trainsfer.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m stylize(model, \u001b[33m\"\u001b[39m\u001b[33mtest_image.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moutput.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
